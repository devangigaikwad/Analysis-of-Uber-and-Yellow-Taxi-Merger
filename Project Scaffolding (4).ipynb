{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1VERPjEZcC1XSs4-02aM-DbkNr_yaJVbFjLJxaYQswqA/edit#)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_Anything in italics (prose) or comments (in code) is meant to provide you with guidance. **Remove the italic lines and provided comments** before submitting the project, if you choose to use this scaffolding. We don't need the guidance when grading._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only a suggestion at the approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25627e8d",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project._\n",
    "\n",
    "* Code clarity: make sure the code conforms to:\n",
    "    * [ ] [PEP 8](https://peps.python.org/pep-0008/) - You might find [this resource](https://realpython.com/python-pep8/) helpful as well as [this](https://github.com/dnanhkhoa/nb_black) or [this](https://jupyterlab-code-formatter.readthedocs.io/en/latest/) tool\n",
    "    * [ ] [PEP 257](https://peps.python.org/pep-0257/)\n",
    "    * [ ] Break each task down into logical functions\n",
    "* The following files are submitted for the project (see the project's GDoc for more details):\n",
    "    * [ ] `README.md`\n",
    "    * [ ] `requirements.txt`\n",
    "    * [ ] `.gitignore`\n",
    "    * [ ] `schema.sql`\n",
    "    * [ ] 6 query files (using the `.sql` extension), appropriately named for the purpose of the query\n",
    "    * [x] Jupyter Notebook containing the project (this file!)\n",
    "* [x] You can edit this cell and add a `x` inside the `[ ]` like this task to denote a completed task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "\n",
    "import requests\n",
    "import bs4\n",
    "import re\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from skimpy import clean_columns\n",
    "import sqlalchemy as db\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b622a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any general notebook setup, like log formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need, for example:\n",
    "\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "# add other constants to refer to any local data, e.g. uber & weather\n",
    "UBER_DATA = \"uber_rides_sample.csv\"\n",
    "\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "ny_lat0, ny_lon0 = NEW_YORK_BOX_COORDS[0]\n",
    "ny_lat1, ny_lon1 = NEW_YORK_BOX_COORDS[1]\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf38168",
   "metadata": {},
   "source": [
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project. The order of these tasks aren't necessarily the order in which they need to be done. It's okay to do them in an order that makes sense to you._\n",
    "\n",
    "* [ ] Define a function that calculates the distance between two coordinates in kilometers that **only uses the `math` module** from the standard library.\n",
    "* [ ] Taxi data:\n",
    "    * [ ] Use the `re` module, and the packages `requests`, BeautifulSoup (`bs4`), and (optionally) `pandas` to programmatically download the required CSV files & load into memory.\n",
    "    * You may need to do this one file at a time - download, clean, sample. You can cache the sampling by saving it as a CSV file (and thereby freeing up memory on your computer) before moving onto the next file. \n",
    "* [ ] Weather & Uber data:\n",
    "    * [ ] Download the data manually in the link provided in the project doc.\n",
    "* [ ] All data:\n",
    "    * [ ] Load the data using `pandas`\n",
    "    * [ ] Clean the data, including:\n",
    "        * Remove unnecessary columns\n",
    "        * Remove invalid data points (take a moment to consider what's invalid)\n",
    "        * Normalize column names\n",
    "        * (Taxi & Uber data) Remove trips that start and/or end outside the designated [coordinate box](http://bboxfinder.com/#40.560445,-74.242330,40.908524,-73.717047)\n",
    "    * [ ] (Taxi data) Sample the data so that you have roughly the same amount of data points over the given date range for both Taxi data and Uber data.\n",
    "* [ ] Weather data:\n",
    "    * [ ] Split into two `pandas` DataFrames: one for required hourly data, and one for the required daily daya.\n",
    "    * [ ] You may find that the weather data you need later on does not exist at the frequency needed (daily vs hourly). You may calculate/generate samples from one to populate the other. Just document what youâ€™re doing so we can follow along. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7499f6d",
   "metadata": {},
   "source": [
    "### Data lookup from taxi_zones.shp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39579c29",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "DriverError",
     "evalue": "taxi_zones.shp: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32mfiona\\_shim.pyx:83\u001b[0m, in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mfiona\\_err.pyx:291\u001b[0m, in \u001b[0;36mfiona._err.exc_wrap_pointer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mCPLE_OpenFailedError\u001b[0m: taxi_zones.shp: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mDriverError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m taxi_zone \u001b[39m=\u001b[39m gpd\u001b[39m.\u001b[39;49mread_file(\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtaxi_zones.shp\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\geopandas\\io\\file.py:259\u001b[0m, in \u001b[0;36m_read_file\u001b[1;34m(filename, bbox, mask, rows, engine, **kwargs)\u001b[0m\n\u001b[0;32m    256\u001b[0m     path_or_bytes \u001b[39m=\u001b[39m filename\n\u001b[0;32m    258\u001b[0m \u001b[39mif\u001b[39;00m engine \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfiona\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 259\u001b[0m     \u001b[39mreturn\u001b[39;00m _read_file_fiona(\n\u001b[0;32m    260\u001b[0m         path_or_bytes, from_bytes, bbox\u001b[39m=\u001b[39mbbox, mask\u001b[39m=\u001b[39mmask, rows\u001b[39m=\u001b[39mrows, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    261\u001b[0m     )\n\u001b[0;32m    262\u001b[0m \u001b[39melif\u001b[39;00m engine \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpyogrio\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    263\u001b[0m     \u001b[39mreturn\u001b[39;00m _read_file_pyogrio(\n\u001b[0;32m    264\u001b[0m         path_or_bytes, bbox\u001b[39m=\u001b[39mbbox, mask\u001b[39m=\u001b[39mmask, rows\u001b[39m=\u001b[39mrows, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    265\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\geopandas\\io\\file.py:303\u001b[0m, in \u001b[0;36m_read_file_fiona\u001b[1;34m(path_or_bytes, from_bytes, bbox, mask, rows, where, **kwargs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     reader \u001b[39m=\u001b[39m fiona\u001b[39m.\u001b[39mopen\n\u001b[0;32m    302\u001b[0m \u001b[39mwith\u001b[39;00m fiona_env():\n\u001b[1;32m--> 303\u001b[0m     \u001b[39mwith\u001b[39;00m reader(path_or_bytes, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mas\u001b[39;00m features:\n\u001b[0;32m    304\u001b[0m         crs \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mcrs_wkt\n\u001b[0;32m    305\u001b[0m         \u001b[39m# attempt to get EPSG code\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\fiona\\env.py:408\u001b[0m, in \u001b[0;36mensure_env_with_credentials.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    406\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    407\u001b[0m     \u001b[39mif\u001b[39;00m local\u001b[39m.\u001b[39m_env:\n\u001b[1;32m--> 408\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    409\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    410\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], \u001b[39mstr\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\fiona\\__init__.py:278\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m     path \u001b[39m=\u001b[39m parse_path(fp)\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 278\u001b[0m     c \u001b[39m=\u001b[39m Collection(path, mode, driver\u001b[39m=\u001b[39mdriver, encoding\u001b[39m=\u001b[39mencoding,\n\u001b[0;32m    279\u001b[0m                    layer\u001b[39m=\u001b[39mlayer, enabled_drivers\u001b[39m=\u001b[39menabled_drivers, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    280\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    281\u001b[0m     \u001b[39mif\u001b[39;00m schema:\n\u001b[0;32m    282\u001b[0m         \u001b[39m# Make an ordered dict of schema properties.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\fiona\\collection.py:162\u001b[0m, in \u001b[0;36mCollection.__init__\u001b[1;34m(self, path, mode, driver, schema, crs, encoding, layer, vsi, archive, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, **kwargs)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    161\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession \u001b[39m=\u001b[39m Session()\n\u001b[1;32m--> 162\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession\u001b[39m.\u001b[39mstart(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    163\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    164\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession \u001b[39m=\u001b[39m WritingSession()\n",
      "File \u001b[1;32mfiona\\ogrext.pyx:540\u001b[0m, in \u001b[0;36mfiona.ogrext.Session.start\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mfiona\\_shim.pyx:90\u001b[0m, in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mDriverError\u001b[0m: taxi_zones.shp: No such file or directory"
     ]
    }
   ],
   "source": [
    "taxi_zone = gpd.read_file(r\"taxi_zones.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61df7f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1618/3630155683.py:3: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  taxi_zone[\"lon\"] = taxi_zone.centroid.x\n",
      "/tmp/ipykernel_1618/3630155683.py:4: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  taxi_zone[\"lat\"] = taxi_zone.centroid.y\n"
     ]
    }
   ],
   "source": [
    "# Get the center of each polygon\n",
    "taxi_zone = taxi_zone.to_crs(4326)\n",
    "taxi_zone[\"lon\"] = taxi_zone.centroid.x\n",
    "taxi_zone[\"lat\"] = taxi_zone.centroid.y\n",
    "\n",
    "# make sure that the coodinate is located inside NY bounding box\n",
    "taxi_zone = taxi_zone[(taxi_zone[\"lon\"] > ny_lon0) & (taxi_zone[\"lon\"] < ny_lon1)]\n",
    "taxi_zone = taxi_zone[(taxi_zone[\"lat\"] > ny_lon1) & (taxi_zone[\"lat\"] < ny_lat1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b29425",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LocationID</th>\n",
       "      <th>coordinate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>(40.69183120640149, -74.17400027276298)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>(40.61674529165988, -73.83129854302214)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>(40.86447368477543, -73.84742223236718)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>(40.72375214158478, -73.97696825691767)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>(40.552659286945655, -74.18848410184931)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>258</td>\n",
       "      <td>(40.897932025294715, -73.8522154537012)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>259</td>\n",
       "      <td>(40.74423471780149, -73.90630644197886)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>260</td>\n",
       "      <td>(40.70913894067186, -74.01302277174901)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>261</td>\n",
       "      <td>(40.77593240314995, -73.94651035601467)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>262</td>\n",
       "      <td>(40.77876585543437, -73.951009874818)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>261 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     LocationID                                coordinate\n",
       "0             1   (40.69183120640149, -74.17400027276298)\n",
       "1             2   (40.61674529165988, -73.83129854302214)\n",
       "2             3   (40.86447368477543, -73.84742223236718)\n",
       "3             4   (40.72375214158478, -73.97696825691767)\n",
       "4             5  (40.552659286945655, -74.18848410184931)\n",
       "..          ...                                       ...\n",
       "257         258   (40.897932025294715, -73.8522154537012)\n",
       "258         259   (40.74423471780149, -73.90630644197886)\n",
       "259         260   (40.70913894067186, -74.01302277174901)\n",
       "260         261   (40.77593240314995, -73.94651035601467)\n",
       "261         262     (40.77876585543437, -73.951009874818)\n",
       "\n",
       "[261 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create dataframe to handle location data\n",
    "df_taxi_zone = taxi_zone[[\"LocationID\"]].copy()\n",
    "df_taxi_zone[\"coordinate\"] = pd.Series(zip(taxi_zone.lat, taxi_zone.lon))\n",
    "\n",
    "df_taxi_zone = df_taxi_zone.dropna()\n",
    "\n",
    "df_taxi_zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d7da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlat_result = []\\nlon_result = []\\n\\nfor data in range(len(df_location.coordinate)):\\n    lat, lon = df_location[\"coordinate\"].iloc[data]\\n    \\n    lat_result.append(lat)\\n    lon_result.append(lon)\\n\\nmin_lat = min(lat_result)\\nmax_lat = max(lat_result)\\nmin_lon = min(lon_result)\\nmax_lon = max(lon_result)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Validate of NY bounding box (df_location)\n",
    "\"\"\"\n",
    "lat_result = []\n",
    "lon_result = []\n",
    "\n",
    "for data in range(len(df_location.coordinate)):\n",
    "    lat, lon = df_location[\"coordinate\"].iloc[data]\n",
    "    \n",
    "    lat_result.append(lat)\n",
    "    lon_result.append(lon)\n",
    "\n",
    "min_lat = min(lat_result)\n",
    "max_lat = max(lat_result)\n",
    "min_lon = min(lon_result)\n",
    "max_lon = max(lon_result)\n",
    "\"\"\"\n",
    "# Note: NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "# min_lon, max_lon = (-74.23353354937652, -73.72665537528711)\n",
    "# min_lat, max_lat = (40.525494581763816, 40.89952906437648)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculating distance\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(from_coord, to_coord):\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for data in range(max(len(from_coord), len(to_coord))):\n",
    "        lat1, lon1 = from_coord[data]\n",
    "        lat2, lon2 = to_coord[data]\n",
    "                      \n",
    "        #method to convert a degree value into radians\n",
    "        lon1 = math.radians(lon1)\n",
    "        lon2 = math.radians(lon2)\n",
    "        lat1 = math.radians(lat1)\n",
    "        lat2 = math.radians(lat2)\n",
    "        \n",
    "        #Haversine Formula\n",
    "        diff_lon = lon2 - lon1 \n",
    "        diff_lat = lat2 - lat1\n",
    "        \n",
    "        a = math.sin(diff_lat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(diff_lon / 2)**2\n",
    "        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "        \n",
    "        # Radius of earth in kilometers. Use 3956 for miles\n",
    "        r = 6371\n",
    "                      \n",
    "        #calculate the result\n",
    "        distance = r*c\n",
    "        result.append(distance)\n",
    "    \n",
    "    distance_series = pd.Series(result)\n",
    "                      \n",
    "    return distance_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6abf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_column(dataframe):\n",
    "    #merge main dataframe with the df_location based on Pick-up Location\n",
    "    dataframe = dataframe.merge(df_taxi_zone, left_on='PULocationID', right_on='LocationID')\n",
    "    dataframe = dataframe.rename(columns={'coordinate':'PUCoordinate'})\n",
    "    \n",
    "    #merge main dataframe with the df_location based on Drop-off Location\n",
    "    dataframe = dataframe.merge(df_taxi_zone, left_on='DOLocationID', right_on='LocationID')\n",
    "    dataframe = dataframe.rename(columns={'coordinate':'DOCoordinate'})\n",
    "\n",
    "    dataframe = dataframe.drop(columns=[\"LocationID_x\",\"LocationID_y\"])\n",
    "    \n",
    "    dataframe[\"distance\"] = calculate_distance(dataframe[\"PUCoordinate\"], dataframe[\"DOCoordinate\"])\n",
    "    \n",
    "    dataframe = dataframe.dropna()\n",
    "    \n",
    "    return dataframe[\"distance\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Processing Taxi Data\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_taxi_csv_urls():\n",
    "    #Get taxi HTML\n",
    "    response = requests.get(TAXI_URL)\n",
    "    html = response.content\n",
    "    \n",
    "    #Find taxi parquet links\n",
    "    soup = bs4.BeautifulSoup(html, 'html.parser')\n",
    "    result = [a['href'] for a in soup.find_all('a', title=\"Yellow Taxi Trip Records\")]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month_taxi_data(url):\n",
    "    dataframe = pd.DataFrame(None)\n",
    "    match = re.search(r\"\\w+2014-01.parquet\", url) #keep in mind that currently we only take 2022-01 data to make the preprocessing more efficient, we can update the REGEX once we wrapped up all the sections\n",
    "    if match is None:\n",
    "        pass\n",
    "    else:\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        with open(match.group(), \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024): \n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    \n",
    "        with open(match.group(), \"r\") as f:\n",
    "            dataframe = pd.read_parquet(match.group(), engine='pyarrow')\n",
    "            #dataframe = dataframe.sample(frac=0.01, random_state=1)\n",
    "            dataframe = dataframe[['tpep_pickup_datetime','PULocationID', 'DOLocationID','total_amount']]\n",
    "            dataframe[\"distance\"] = add_distance_column(dataframe)\n",
    "            dataframe[\"tpep_pickup_date\"] = pd.to_datetime(dataframe[\"tpep_pickup_datetime\"]).dt.date\n",
    "            dataframe[\"tpep_pickup_hour\"] = pd.to_datetime(dataframe[\"tpep_pickup_datetime\"]).dt.hour\n",
    "            dataframe[\"tpep_pickup_day\"] = pd.to_datetime(dataframe[\"tpep_pickup_datetime\"]).dt.day_name()\n",
    "            dataframe = dataframe[['tpep_pickup_date','tpep_pickup_day','tpep_pickup_hour','distance']]\n",
    "            dataframe = dataframe.dropna(subset=[\"distance\"])\n",
    "            \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39751235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data():\n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    all_csv_urls = find_taxi_csv_urls()\n",
    "    for csv_url in all_csv_urls:\n",
    "        # maybe: first try to see if you've downloaded this exact\n",
    "        # file already and saved it before trying again\n",
    "        dataframe = get_and_clean_month_taxi_data(csv_url)\n",
    "        \n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "        \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    \n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(csv_file):\n",
    "    df_uber = pd.read_csv(csv_file)\n",
    "    \n",
    "    #renaming old df without creating a copy \n",
    "    df_uber.rename(columns={'key': 'date_time', 'Unnamed: 0': 'location_id'}, inplace=True)\n",
    "    \n",
    "    #constructing new columns as a function input\n",
    "    df_uber[\"pickup_coordinate\"] = list(zip(df_uber.pickup_latitude, df_uber.pickup_longitude))\n",
    "    df_uber[\"dropoff_coordinate\"] = list(zip(df_uber.dropoff_latitude, df_uber.dropoff_longitude))\n",
    "    \n",
    "    #calculate distance\n",
    "    df_uber[\"distance\"] = calculate_distance(df_uber[\"pickup_coordinate\"], df_uber[\"dropoff_coordinate\"])\n",
    "    \n",
    "    #make sure that the coordinate is located inside the NY bounding box\n",
    "    df_uber = df_uber[(df_uber[\"pickup_longitude\"] > ny_lon0) & (df_uber[\"pickup_longitude\"] < ny_lon1)]\n",
    "    df_uber = df_uber[(df_uber[\"dropoff_longitude\"] > ny_lon0) & (df_uber[\"dropoff_longitude\"] < ny_lon1)]\n",
    "    df_uber = df_uber[(df_uber[\"pickup_latitude\"] > ny_lat0) & (df_uber[\"pickup_latitude\"] < ny_lat1)]\n",
    "    df_uber = df_uber[(df_uber[\"dropoff_latitude\"] > ny_lat0) & (df_uber[\"dropoff_latitude\"] < ny_lat1)]\n",
    "    \n",
    "    #handle the datetime format\n",
    "    df_uber[\"order_date\"] = pd.to_datetime(df_uber[\"date_time\"]).dt.date\n",
    "    df_uber[\"order_time\"] = pd.to_datetime(df_uber[\"date_time\"]).dt.hour\n",
    "    df_uber[\"order_day\"] = pd.to_datetime(df_uber[\"date_time\"]).dt.day_name()\n",
    "    \n",
    "    df_uber[\"pickup_date\"] = pd.to_datetime(df_uber[\"pickup_datetime\"]).dt.date\n",
    "    df_uber[\"pickup_time\"] = pd.to_datetime(df_uber[\"pickup_datetime\"]).dt.hour\n",
    "    df_uber[\"pickup_day\"] = pd.to_datetime(df_uber[\"pickup_datetime\"]).dt.day_name()\n",
    "    \n",
    "    df_uber = df_uber.drop(columns=[\"date_time\", \"pickup_datetime\"])\n",
    "    \n",
    "    return df_uber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    uber_dataframe = load_and_clean_uber_data(UBER_DATA)\n",
    "    uber_dataframe = uber_dataframe.drop(columns=[\"pickup_coordinate\",\"dropoff_coordinate\",\n",
    "                                                  \"pickup_longitude\", \"pickup_latitude\", \n",
    "                                                  \"dropoff_longitude\", \"dropoff_latitude\"])\n",
    "    \n",
    "    return uber_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e864ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    df_weather = pd.read_csv(csv_file, low_memory = False)\n",
    "    weather_columns = df_weather.columns.tolist()\n",
    "    result = ['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'ELEVATION', 'NAME', 'REPORT_TYPE', 'SOURCE']\n",
    "    \n",
    "    for column in weather_columns:\n",
    "        match = re.search(r\"Hourly\", column)\n",
    "        if match is None:\n",
    "            pass\n",
    "        else:\n",
    "            result.append(column)\n",
    "    \n",
    "    df_weather_hourly = df_weather[result]\n",
    "    \n",
    "    return df_weather_hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    df_weather = pd.read_csv(csv_file, low_memory = False)\n",
    "    weather_columns = df_weather.columns.tolist()\n",
    "    result = ['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'ELEVATION', 'NAME', 'REPORT_TYPE', 'SOURCE']\n",
    "    \n",
    "    for column in weather_columns:\n",
    "        match = re.search(r\"Daily\", column)\n",
    "        if match is None:\n",
    "            pass\n",
    "        else:\n",
    "            result.append(column)\n",
    "    \n",
    "    df_weather_daily = df_weather[result]\n",
    "    \n",
    "    return df_weather_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "    \n",
    "    # add some way to find all weather CSV files\n",
    "    weather_csv_files = []\n",
    "    list_dir = os.listdir() #list all file within a directory and store in an array\n",
    "\n",
    "    for file in list_dir:\n",
    "        match = re.search(r\"(2009|201[012345])_weather.csv\", file)\n",
    "        if match is None:\n",
    "            pass\n",
    "        else:\n",
    "            weather_csv_files.append(match.group())\n",
    "    \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    #handle datetime format - hourly data\n",
    "    hourly_data[\"ORDER_DATE\"] = pd.to_datetime(hourly_data[\"DATE\"]).dt.date\n",
    "    hourly_data[\"ORDER_TIME\"] = pd.to_datetime(hourly_data[\"DATE\"]).dt.hour\n",
    "    hourly_data[\"ORDER_DAY\"] = pd.to_datetime(hourly_data[\"DATE\"]).dt.day_name()\n",
    "    hourly_data = hourly_data.drop(columns=[\"DATE\"])\n",
    "    \n",
    "    #handle datetime format - daily data\n",
    "    daily_data[\"ORDER_DATE\"] = pd.to_datetime(daily_data[\"DATE\"]).dt.date\n",
    "    daily_data[\"ORDER_TIME\"] = pd.to_datetime(daily_data[\"DATE\"]).dt.hour\n",
    "    daily_data[\"ORDER_DAY\"] = pd.to_datetime(daily_data[\"DATE\"]).dt.day_name()\n",
    "    daily_data = daily_data.drop(columns=[\"DATE\"])\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f900f7aa",
   "metadata": {},
   "source": [
    "### Process All Data\n",
    "\n",
    "_This is where you can actually execute all the required functions._\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data = get_and_clean_taxi_data()\n",
    "uber_data = get_uber_data()\n",
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fada2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000000     715024\n",
       "2.642271     117207\n",
       "15.907308     61580\n",
       "14.746516     59296\n",
       "3.894181      56181\n",
       "              ...  \n",
       "8.427991          1\n",
       "10.822885         1\n",
       "13.728395         1\n",
       "12.737658         1\n",
       "2.687064          1\n",
       "Name: distance, Length: 18181, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_data.distance.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2: Storing Cleaned Data\n",
    "\n",
    "_Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b8149a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Engine(sqlite:///project.db)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine = db.create_engine(DATABASE_URL)\n",
    "engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather\n",
    "(\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    STATION INTEGER,\n",
    "    LATITUDE REAL,\n",
    "    LONGITUDE REAL,\n",
    "    ELEVATION REAL,\n",
    "    NAME TEXT,\n",
    "    REPORT_TYPE TEXT,\n",
    "    SOURCE TEXT,\n",
    "    HourlyAltimeterSetting REAL,\n",
    "    HourlyDewPointTemperature REAL,\n",
    "    HourlyDryBulbTemperature REAL,\n",
    "    HourlyPrecipitation REAL,\n",
    "    HourlyPresentWeatherType REAL,\n",
    "    HourlyPressureChange REAL,\n",
    "    HourlyPressureTendency REAL,\n",
    "    HourlyRelativeHumidity REAL,\n",
    "    HourlySkyConditions REAL,\n",
    "    HourlySeaLevelPressure REAL,\n",
    "    HourlyStationPressure REAL,\n",
    "    HourlyVisibility REAL,\n",
    "    HourlyWetBulbTemperature REAL,\n",
    "    HourlyWindDirection REAL,\n",
    "    HourlyWindGustSpeed REAL,\n",
    "    HourlyWindSpeed REAL,\n",
    "    ORDER_DATE DATE,\n",
    "    ORDER_TIME INTEGER,\n",
    "    ORDER_DAY STRING\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather\n",
    "(\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    STATION INTEGER,\n",
    "    LATITUDE REAL,\n",
    "    LONGITUDE REAL,\n",
    "    ELEVATION REAL,\n",
    "    NAME TEXT,\n",
    "    REPORT_TYPE TEXT,\n",
    "    SOURCE TEXT,\n",
    "    DailyAverageDewPointTemperature REAL,\n",
    "    DailyAverageDryBulbTemperature REAL,\n",
    "    DailyAverageRelativeHumidity REAL,\n",
    "    DailyAverageSeaLevelPressure REAL, \n",
    "    DailyAverageStationPressure REAL,  \n",
    "    DailyAverageWetBulbTemperature REAL,\n",
    "    DailyAverageWindSpeed REAL,        \n",
    "    DailyCoolingDegreeDays REAL,       \n",
    "    DailyDepartureFromNormalAverageTemperature REAL,\n",
    "    DailyHeatingDegreeDays REAL,        \n",
    "    DailyMaximumDryBulbTemperature REAL,\n",
    "    DailyMinimumDryBulbTemperature REAL,\n",
    "    DailyPeakWindDirection REAL,        \n",
    "    DailyPeakWindSpeed REAL,            \n",
    "    DailyPrecipitation REAL,            \n",
    "    DailySnowDepth REAL,                \n",
    "    DailySnowfall REAL,                 \n",
    "    DailySustainedWindDirection REAL,   \n",
    "    DailySustainedWindSpeed REAL,       \n",
    "    DailyWeather TEXT,\n",
    "    ORDER_DATE DATE,\n",
    "    ORDER_TIME INTEGER,\n",
    "    ORDER_DAY STRING\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxi_trips\n",
    "(\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    tpep_pickup_date DATE,\n",
    "    tpep_pickup_day STRING,\n",
    "    tpep_pickup_hour INTEGER,\n",
    "    distance REAL\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_trips\n",
    "(\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    location_id BIGINT,\n",
    "    fare_amount REAL,\n",
    "    pickup_datetime TEXT,\n",
    "    passenger_count INTEGER,\n",
    "    distance REAL,\n",
    "    order_date DATE,\n",
    "    order_time INTEGER,\n",
    "    order_day STRING,\n",
    "    pickup_date DATE,\n",
    "    pickup_time INTEGER,\n",
    "    pickup_day STRING\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    connection.execute(UBER_TRIPS_SCHEMA)\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    for key, value in table_to_df_dict.items():\n",
    "        value.to_sql(key, engine, index=True, index_label=\"id\", if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_weather_data,\n",
    "    \"daily_weather\": daily_weather_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4753fcd",
   "metadata": {},
   "source": [
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project. The order of these tasks aren't necessarily the order in which they need to be done. It's okay to do them in an order that makes sense to you._\n",
    "\n",
    "* [ ] For 01-2009 through 06-2015, what hour of the day was the most popular to take a yellow taxi? The result should have 24 bins.\n",
    "* [ ] For the same time frame, what day of the week was the most popular to take an uber? The result should have 7 bins.\n",
    "* [ ] What is the 95% percentile of distance traveled for all hired trips during July 2013?\n",
    "* [ ] What were the top 10 days with the highest number of hired rides for 2009, and what was the average distance for each day?\n",
    "* [ ] Which 10 days in 2014 were the windiest, and how many hired trips were made on those days?\n",
    "* [ ] During Hurricane Sandy in NYC (Oct 29-30, 2012) and the week leading up to it, how many trips were taken each hour, and for each hour, how much precipitation did NYC receive and what was the sustained wind speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_query_to_file(query, outfile):\n",
    "    raise NotImplemented()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query N\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._\n",
    "\n",
    "_Repeat for each query_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONT FORGET TO ADJUST THE pickup date to BETWEEN 2009-01-01 AND 2015-06-30\n",
    "QUERY_1 = \"\"\"\n",
    "SELECT tpep_pickup_hour as order_hour,\n",
    "    COUNT(id) as num_order    \n",
    "FROM taxi_trips\n",
    "WHERE tpep_pickup_date BETWEEN \"2014-01-01\" AND \"2014-01-31\" \n",
    "GROUP BY order_hour\n",
    "ORDER BY num_order DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a1ea96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(19, 806028),\n",
       " (18, 804625),\n",
       " (20, 736978),\n",
       " (21, 704657),\n",
       " (22, 685062),\n",
       " (14, 681545),\n",
       " (17, 675791),\n",
       " (15, 660131),\n",
       " (12, 658526),\n",
       " (13, 653324),\n",
       " (11, 618124),\n",
       " (9, 617279),\n",
       " (10, 603632),\n",
       " (8, 599800),\n",
       " (23, 592199),\n",
       " (16, 567030),\n",
       " (0, 495845),\n",
       " (7, 482256),\n",
       " (1, 370586),\n",
       " (2, 286356),\n",
       " (6, 275027),\n",
       " (3, 212026),\n",
       " (4, 155767),\n",
       " (5, 133158)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(QUERY_1).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd97a9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONT FORGET TO ADJUST THE pickup date to BETWEEN 2009-01-01 AND 2015-06-30\n",
    "QUERY_2 = \"\"\"\n",
    "SELECT order_day,\n",
    "    COUNT(id) as num_order    \n",
    "FROM uber_trips\n",
    "WHERE order_date BETWEEN \"2009-01-01\" AND \"2015-06-30\" \n",
    "GROUP BY order_day\n",
    "ORDER BY num_order DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48938d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Friday', 30166),\n",
       " ('Saturday', 29599),\n",
       " ('Thursday', 29338),\n",
       " ('Wednesday', 28328),\n",
       " ('Tuesday', 27526),\n",
       " ('Sunday', 25834),\n",
       " ('Monday', 24681)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(QUERY_2).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b101dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONT FORGET TO ADJUST THE pickup date to in July 2013\n",
    "#Still confuse wheteher to use NTILE(95) or NTILE(5)\n",
    "QUERY_3 = \"\"\"\n",
    "WITH p AS (SELECT distance, NTILE(95) OVER (ORDER BY distance) AS percentile\n",
    "           FROM taxi_trips\n",
    "           WHERE tpep_pickup_date BETWEEN \"2014-01-01\" AND \"2014-01-31\")\n",
    "SELECT MAX(distance) as distance\n",
    "FROM p\n",
    "GROUP BY percentile;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a0a907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0,),\n",
       " (0.0,),\n",
       " (0.0,),\n",
       " (0.0,),\n",
       " (0.0,),\n",
       " (0.6285024183261585,),\n",
       " (0.9655440531449592,),\n",
       " (1.0562774656645701,),\n",
       " (1.1253421532439245,),\n",
       " (1.2527751392551087,),\n",
       " (1.4283302836383676,),\n",
       " (1.4644882545106062,),\n",
       " (1.5362636333263973,),\n",
       " (1.5651276523999635,),\n",
       " (1.6705008987214616,),\n",
       " (1.8744031469935596,),\n",
       " (2.010893187566884,),\n",
       " (2.1218976321629133,),\n",
       " (2.158174956836439,),\n",
       " (2.288227936332771,),\n",
       " (2.4050510585176283,),\n",
       " (2.589657516582127,),\n",
       " (2.6422711045844665,),\n",
       " (2.7470224403268917,),\n",
       " (2.875779451258252,),\n",
       " (2.960576344339237,),\n",
       " (3.0575673335662588,),\n",
       " (3.234100298255203,),\n",
       " (3.4740555997415443,),\n",
       " (3.6917429782369253,),\n",
       " (3.818760570708154,),\n",
       " (3.9238445795231285,),\n",
       " (4.115530149053466,),\n",
       " (4.311835711937714,),\n",
       " (4.459282023857271,),\n",
       " (4.760397081292544,),\n",
       " (5.044797566460685,),\n",
       " (5.261389595635906,),\n",
       " (5.44100227429195,),\n",
       " (5.6952863407275185,),\n",
       " (5.961143366070103,),\n",
       " (6.185845986918711,),\n",
       " (6.59607386042849,),\n",
       " (6.819887571061227,),\n",
       " (7.33778935756258,),\n",
       " (7.750096354481571,),\n",
       " (8.121455135029509,),\n",
       " (8.483702651347395,),\n",
       " (8.878813699922638,),\n",
       " (9.210115448388926,),\n",
       " (9.5660153040042,),\n",
       " (10.081496212481127,),\n",
       " (10.443287605017238,),\n",
       " (10.936637756043956,),\n",
       " (11.297031088181246,),\n",
       " (11.613313579730374,),\n",
       " (11.809263367613402,),\n",
       " (12.156570461803438,),\n",
       " (12.60007967636859,),\n",
       " (12.942349872645925,),\n",
       " (13.230237140329944,),\n",
       " (13.577674201464465,),\n",
       " (13.773315342128218,),\n",
       " (14.011916420399581,),\n",
       " (14.157062286669941,),\n",
       " (14.339145875954587,),\n",
       " (14.665924200248037,),\n",
       " (14.83809118056881,),\n",
       " (14.997064689901535,),\n",
       " (15.152914684892279,),\n",
       " (15.484860509702333,),\n",
       " (15.61832863598479,),\n",
       " (15.77846242720665,),\n",
       " (15.921917837517794,),\n",
       " (16.186708595327282,),\n",
       " (16.43665390753145,),\n",
       " (16.8753123244313,),\n",
       " (17.16026904253573,),\n",
       " (17.55144539146015,),\n",
       " (17.862044260339037,),\n",
       " (18.369793033390017,),\n",
       " (18.74693031340176,),\n",
       " (19.314778505519868,),\n",
       " (19.665721740610362,),\n",
       " (20.14653354152512,),\n",
       " (20.77012435185092,),\n",
       " (21.492455060799575,),\n",
       " (22.024784190522784,),\n",
       " (22.681842340004028,),\n",
       " (23.29638761048896,),\n",
       " (25.071698245500617,),\n",
       " (26.899342266978724,),\n",
       " (29.10288144753976,),\n",
       " (31.32235805920427,),\n",
       " (49.12409053253548,)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(QUERY_3).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b9ec81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONT FORGET TO ADJUST THE pickup date to BETWEEN 2009-01-01 AND 2009-12-31\n",
    "QUERY_4 = \"\"\"\n",
    "SELECT tpep_pickup_date as order_date,\n",
    "    COUNT(id) as num_order\n",
    "FROM taxi_trips\n",
    "WHERE order_date BETWEEN \"2014-01-01\" AND \"2014-01-31\"\n",
    "GROUP BY order_date\n",
    "ORDER BY num_order DESC\n",
    "LIMIT 10\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9a70af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2014-01-25', 530551),\n",
       " ('2014-01-11', 511172),\n",
       " ('2014-01-24', 509509),\n",
       " ('2014-01-10', 507732),\n",
       " ('2014-01-18', 507033),\n",
       " ('2014-01-17', 490852),\n",
       " ('2014-01-16', 482037),\n",
       " ('2014-01-09', 479714),\n",
       " ('2014-01-29', 477984),\n",
       " ('2014-01-08', 473611)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(QUERY_4).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780ba9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONT FORGET TO ADJUST THE pickup date to BETWEEN 2014-01-01 to 2014-12-31\n",
    "QUERY_5 = \"\"\"\n",
    "WITH taxi_trip AS (\n",
    "    SELECT tpep_pickup_date as order_date,\n",
    "        COUNT(id) as num_order    \n",
    "    FROM taxi_trips\n",
    "    WHERE tpep_pickup_date BETWEEN \"2014-01-01\" AND \"2014-01-31\"\n",
    "    GROUP BY order_date\n",
    "    ORDER BY num_order DESC\n",
    "    ),\n",
    "    daily_observation_data AS (\n",
    "    SELECT ORDER_DATE as order_date,\n",
    "        AVG(DailyAverageWindSpeed) as avg_wind_speed\n",
    "    FROM daily_weather\n",
    "    WHERE order_date BETWEEN \"2014-01-01\" AND \"2014-01-31\"\n",
    "    GROUP BY order_date\n",
    "    ORDER BY avg_wind_speed DESC\n",
    "    LIMIT 10\n",
    "    )\n",
    "SELECT daily_observation_data.order_date,\n",
    "    daily_observation_data.avg_wind_speed,\n",
    "    taxi_trip.num_order\n",
    "FROM daily_observation_data\n",
    "LEFT JOIN taxi_trip ON daily_observation_data.order_date = taxi_trip.order_date\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f78ddc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2014-01-07', 13.1, 460052),\n",
       " ('2014-01-02', 12.2, 359958),\n",
       " ('2014-01-21', 10.4, 255518),\n",
       " ('2014-01-03', 10.4, 275470),\n",
       " ('2014-01-22', 8.8, 377773),\n",
       " ('2014-01-06', 8.5, 395445),\n",
       " ('2014-01-12', 8.2, 450487),\n",
       " ('2014-01-24', 8.1, 509509),\n",
       " ('2014-01-27', 7.8, 418863),\n",
       " ('2014-01-25', 7.6, 530551)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(QUERY_5).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_N, \"some_descriptive_name.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data\n",
    "\n",
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project. The order of these tasks aren't necessarily the order in which they need to be done. It's okay to do them in an order that makes sense to you._\n",
    "\n",
    "* [ ] Create an appropriate visualization for the first query/question in part 3\n",
    "* [ ] Create a visualization that shows the average distance traveled per month (regardless of year - so group by each month). Include the 90% confidence interval around the mean in the visualization\n",
    "* [ ] Define three lat/long coordinate boxes around the three major New York airports: LGA, JFK, and EWR (you can use bboxfinder to help). Create a visualization that compares what day of the week was most popular for drop offs for each airport.\n",
    "* [ ] Create a heatmap of all hired trips over a map of the area. Consider using KeplerGL or another library that helps generate geospatial visualizations.\n",
    "* [ ] Create a scatter plot that compares tip amount versus distance.\n",
    "* [ ] Create another scatter plot that compares tip amount versus precipitation amount.\n",
    "\n",
    "_Be sure these cells are executed so that the visualizations are rendered when the notebook is submitted._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization N\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._\n",
    "\n",
    "_Repeat for each visualization._\n",
    "\n",
    "_The example below makes use of the `matplotlib` library. There are other libraries, including `pandas` built-in plotting library, kepler for geospatial data representation, `seaborn`, and others._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_n(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_n():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_n()\n",
    "plot_visual_n(some_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0072dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06441971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a903c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sqlalchemy\n",
      "  Downloading SQLAlchemy-1.4.44-cp310-cp310-win_amd64.whl (1.6 MB)\n",
      "     ---------------------------------------- 1.6/1.6 MB 8.3 MB/s eta 0:00:00\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-2.0.1-cp310-cp310-win_amd64.whl (190 kB)\n",
      "     -------------------------------------- 190.9/190.9 kB 5.6 MB/s eta 0:00:00\n",
      "Installing collected packages: greenlet, sqlalchemy\n",
      "Successfully installed greenlet-2.0.1 sqlalchemy-1.4.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: C:\\Users\\devan\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871ae164",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "9963f962fa0dba6e28c55a755360ca0da4578659b2faf85164cd246ed6641bd1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
